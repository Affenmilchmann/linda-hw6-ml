{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "from sys import getsizeof\n",
    "from time import time\n",
    "\n",
    "def mb_size_str(obj: object, name: str):\n",
    "    return f\"{name} size is {round(getsizeof(obj) / pow(1024, 2), 2)}MB\"\n",
    "\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as StopWords\n",
    "\n",
    "stop_words = set(StopWords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "train_df = pd.read_csv(train_file)\n",
    "TRAIN_DATASET_SIZE = train_df.shape[0]\n",
    "b_time = time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'DO_TFIDF_WEIGHTING': False,\n",
    "    'DO_LEMMATIZING': False,\n",
    "    'DO_STEMMING': False,\n",
    "    'DO_FILTER_STOPWORDS': True,\n",
    "    'DO_FILTER_LOW_FREQ': True,\n",
    "    'LOWFREQ_TRESHOLD': 3,\n",
    "    'MODEL': \"6 Gensim Continuous Skipgram\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:27<00:00, 894.39it/s] \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse._csr import csr_matrix\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "word_freq = {}\n",
    "total_tokens = 0\n",
    "\n",
    "def token_form_handlers(tokens: list[str]) -> None:\n",
    "    \"\"\"These are done in place\"\"\"\n",
    "    if not (cfg['DO_LEMMATIZING'] or cfg['DO_STEMMING']): return\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if cfg['DO_LEMMATIZING']:\n",
    "            tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "        if cfg['DO_STEMMING']: \n",
    "            result = wn.morphy(tokens[i])\n",
    "            if result:\n",
    "                tokens[i] = result\n",
    "\n",
    "def stopword_filtering(tokens: list[str]) -> list[str]:\n",
    "    if not cfg['DO_FILTER_STOPWORDS']: return tokens\n",
    "    return [tkn for tkn in tokens if not tkn in stop_words]\n",
    "\n",
    "def count_words(tokens: list[str]) -> None:\n",
    "    global total_tokens\n",
    "    for t in tokens:\n",
    "        total_tokens += 1\n",
    "        if t in word_freq: word_freq[t] += 1\n",
    "        else: word_freq[t] = 1\n",
    "\n",
    "def preproc(text: str) -> str:\n",
    "    # regex removes all enclosed in <> (html tags) and any non letter non space characters\n",
    "    # then collapses all space characters to a single space\n",
    "    text = re.sub(f'\\s+', ' ', re.sub(r'(<[^>]*>)|[^\\w\\s]', '', text.lower()))\n",
    "    tokens = text.split(' ')\n",
    "    token_form_handlers(tokens)\n",
    "    tokens = stopword_filtering(tokens)\n",
    "    count_words(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "#           Preprocessing           #\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "print(\"Preprocessing...\")\n",
    "for row_id in tqdm(range(TRAIN_DATASET_SIZE)):\n",
    "    train_df.loc[row_id, \"text\"] = preproc(train_df.loc[row_id, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing low freq tokens (freq <= 3)...\n",
      "Total forms: 144613\n",
      "Total forms to be removed 108752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:12<00:00, 2076.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token amount:\n",
      "\told 3014192\n",
      "\tnew 2875149\n",
      "4.61% total less\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "forms_to_remove = set(w for (w, freq) in word_freq.items() if freq <= cfg['LOWFREQ_TRESHOLD'])\n",
    "\n",
    "new_total_tokens = 0\n",
    "def remove_tokens(text: str) -> str:\n",
    "    global new_total_tokens\n",
    "    new_tokens = [tkn for tkn in text.split() if not tkn in forms_to_remove]\n",
    "    new_total_tokens += len(new_tokens)\n",
    "    return ' '.join(new_tokens)\n",
    "\n",
    "print(f\"Removing low freq tokens (freq <= {cfg['LOWFREQ_TRESHOLD']})...\")\n",
    "print(f\"Total forms: {len(word_freq.items())}\")\n",
    "print(f\"Total forms to be removed {len(forms_to_remove)}\")\n",
    "texts = []\n",
    "for row_id in tqdm(range(TRAIN_DATASET_SIZE)):\n",
    "    train_df.loc[row_id, \"text\"] = remove_tokens(train_df.loc[row_id, \"text\"])\n",
    "    texts.append(train_df.loc[row_id, \"text\"])\n",
    "shrink_percent = round(100 * (total_tokens - new_total_tokens) / total_tokens, 2)\n",
    "print(f\"Total token amount:\\n\\told {total_tokens}\\n\\tnew {new_total_tokens}\\n{shrink_percent}% total less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f\"train_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DO_TFIDF_WEIGHTING = False\n"
     ]
    }
   ],
   "source": [
    "if cfg['DO_TFIDF_WEIGHTING']:\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "    logger.info(\"TF-IDF matrix...\")\n",
    "    matrix: csr_matrix = tfidf.fit_transform(texts)\n",
    "    print(f\"Матрица на {matrix.shape[0]} документов и {matrix.shape[1]} термов\")\n",
    "else:\n",
    "    print(f\"DO_TFIDF_WEIGHTING = {cfg['DO_TFIDF_WEIGHTING']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading projection weights from 6 Gensim Continuous Skipgram/model.bin\n",
      "INFO : KeyedVectors lifecycle event {'msg': 'loaded (302866, 300) matrix of type float32 from 6 Gensim Continuous Skipgram/model.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-02-12T18:15:02.210760', 'gensim': '4.3.0', 'python': '3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]', 'platform': 'Linux-6.1.11-1-MANJARO-x86_64-with-glibc2.37', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE MODEL\n",
    "from nltk.tokenize import word_tokenize\n",
    "w2v_file = f\"{cfg['MODEL']}/model.bin\"\n",
    "model: gensim.models.keyedvectors.KeyedVectors = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "VECTOR_SIZE = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "Calculating mean review vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:51<00:00, 488.15it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_tfidf_vector(tokens: list[str], text_id: int) -> np.ndarray:\n",
    "    tfidf_vector = np.ndarray(len(tokens))\n",
    "    # a vector with all the terms\n",
    "    text_raw_vector = matrix[text_id, :]\n",
    "    # leaving only terms that are present in the text\n",
    "    for i in range(len(tokens)):\n",
    "        idx = tfidf.vocabulary_.get(tokens[i])\n",
    "        val = matrix[text_id, idx] if idx else 0\n",
    "        tfidf_vector[i] = val\n",
    "    # vector with tfidf values of each word in the text\n",
    "    return tfidf_vector\n",
    "\n",
    "def text_vector(text: str, text_id: int) -> np.ndarray:\n",
    "    \"\"\"Compute the normalized weighted mean w2v vector for a given text\"\"\"\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token in model]\n",
    "    # Compute the word2vec vectors for each word in the text\n",
    "    vectors = [model.get_vector(tkn) for tkn in tokens]\n",
    "    if cfg['DO_TFIDF_WEIGHTING']:\n",
    "        # Compute the tf-idf values for each word in the text.\n",
    "        tfidf_vals = get_tfidf_vector(tokens, text_id)\n",
    "        # Compute the weighted vectors by multiplying the word2vec vectors by the tf-idf values\n",
    "        weighted_vecs = np.array([vec * factor for vec, factor in zip(vectors, tfidf_vals)])\n",
    "    else:\n",
    "        weighted_vecs = np.array(vectors)\n",
    "\n",
    "    text_sum_vec = np.sum(weighted_vecs, axis=0)\n",
    "    norm_vec = text_sum_vec / np.linalg.norm(text_sum_vec)\n",
    "    \n",
    "    return norm_vec\n",
    "\n",
    "review_vectors = np.empty((TRAIN_DATASET_SIZE, VECTOR_SIZE), dtype=float)\n",
    "print(review_vectors.shape)\n",
    "print(\"Calculating mean review vectors...\")\n",
    "for row_id in tqdm(range(TRAIN_DATASET_SIZE)):\n",
    "    review_vectors[row_id] = text_vector(train_df.loc[row_id, \"text\"], row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.2205810546875 MB\n"
     ]
    }
   ],
   "source": [
    "# trying not to run out of memory on my potatoe (╥﹏╥)\n",
    "print(getsizeof(review_vectors) / pow(1024, 2), \"MB\")\n",
    "#model = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "answers = train_df['answer'].to_numpy()\n",
    "print(review_vectors.shape)\n",
    "print(answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DO_FILTER_LOW_FREQ': True,\n",
      " 'DO_FILTER_STOPWORDS': True,\n",
      " 'DO_LEMMATIZING': False,\n",
      " 'DO_STEMMING': False,\n",
      " 'DO_TFIDF_WEIGHTING': False,\n",
      " 'LOWFREQ_TRESHOLD': 3,\n",
      " 'MODEL': '6 Gensim Continuous Skipgram',\n",
      " 'score': 0.83968,\n",
      " 'time': '1m 43s'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pprint\n",
    "from datetime import timedelta as td\n",
    "from datetime import datetime as dt\n",
    "\n",
    "reg = LogisticRegression().fit(review_vectors, answers)\n",
    "\n",
    "diff = time() - b_time\n",
    "\n",
    "results = {}\n",
    "results.update(cfg)\n",
    "results[\"score\"] = reg.score(review_vectors, answers)\n",
    "results[\"time\"] = \"{:.0f}m {:.0f}s\".format(*divmod(diff, 60))\n",
    "pprint(results)\n",
    "# 0.8396 - stopwords only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:54<00:00, 219.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_vector(text: str) -> np.ndarray:\n",
    "    tokens = [token for token in word_tokenize(text) if token in model]\n",
    "    # Compute the word2vec vectors for each word in the text\n",
    "    vectors = np.array([model.get_vector(tkn) for tkn in tokens])\n",
    "    sum_vector = np.sum(vectors, axis=0)\n",
    "    norm_vector = sum_vector / np.linalg.norm(sum_vector)\n",
    "    return norm_vector\n",
    "\n",
    "test_file = \"test.csv\"\n",
    "test_df = pd.read_csv(test_file)\n",
    "TEST_DATASET_SIZE = test_df.shape[0]\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "#             Processing            #\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "print(\"Processing...\")\n",
    "result = test_df[[\"id\"]].copy()\n",
    "for row_id in tqdm(range(TEST_DATASET_SIZE)):\n",
    "    text = test_df.loc[row_id, \"text\"]\n",
    "    text = preproc(text)\n",
    "    text = remove_tokens(text)\n",
    "    test_df.loc[row_id, \"text\"] = text\n",
    "    result.loc[row_id, \"answer\"] = int(reg.predict([get_vector(text)])[0])\n",
    "test_df.to_csv(f\"test_cleaned.csv\", index=False)\n",
    "result[\"answer\"] = result[\"answer\"].astype(int)\n",
    "result.to_csv(f\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
