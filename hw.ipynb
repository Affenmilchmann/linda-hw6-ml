{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "from time import time\n",
    "\n",
    "def mb_size_str(obj: object, name: str):\n",
    "    return f\"{name} size is {round(getsizeof(obj) / pow(1024, 2), 2)}MB\"\n",
    "\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "train_df = pd.read_csv(train_file)\n",
    "INPUT_DATASET_SIZE = train_df.shape[0]\n",
    "b_time = time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'TF_IDF': True,\n",
    "    'LEMMATIZE': False,\n",
    "    'DO_STEMMING': False,\n",
    "    'STOPLIST': True,\n",
    "    'LOWFREQ_FILTER': False,\n",
    "    'LOWFREQ_TRESHOLD': 0,\n",
    "    'TEST_RATIO': 0.2, # ratio of train samples that will go as test ones\n",
    "    'MODEL': \"6 Gensim Continuous Skipgram\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:12<00:00, 1950.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "#           Preprocessing           #\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "\n",
    "import src.preprocessing as prep\n",
    "\n",
    "freq_dict = {}\n",
    "token_amount_counter = [0]\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "for row_id in tqdm(range(INPUT_DATASET_SIZE)):\n",
    "    text = train_df.loc[row_id, \"text\"]\n",
    "    train_df.loc[row_id, \"text\"] = prep.preprocess(\n",
    "        text, cfg, freq_dict, token_amount_counter\n",
    "    )\n",
    "\n",
    "if cfg['LOWFREQ_FILTER']:\n",
    "    forms_to_remove = set(w for (w, freq) in freq_dict.items() if freq <= cfg['LOWFREQ_TRESHOLD'])\n",
    "    new_token_amount_counter = [0]\n",
    "    print(f\"Removing low freq tokens (freq <= {cfg['LOWFREQ_TRESHOLD']})...\")\n",
    "    print(f\"Total forms: {len(freq_dict.items())}\")\n",
    "    print(f\"Total forms to be removed {len(forms_to_remove)}\")\n",
    "    for row_id in tqdm(range(INPUT_DATASET_SIZE)):\n",
    "        text = train_df.loc[row_id, \"text\"]\n",
    "        train_df.loc[row_id, \"text\"] = prep.remove_lowfreq(text, forms_to_remove, new_token_amount_counter)\n",
    "    shrink_percent = round(100 * (token_amount_counter[0] - new_token_amount_counter[0]) / token_amount_counter[0], 2)\n",
    "    print(f\"Total token amount:\\nold {token_amount_counter[0]}\\nnew {new_token_amount_counter[0]}\\n{shrink_percent}% total less\")\n",
    "\n",
    "train_df.to_csv(f\"train_cleaned.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : TF-IDF matrix...\n",
      "INFO : Матрица на 25000 документов и 144380 термов\n"
     ]
    }
   ],
   "source": [
    "from src.tfidf import get_matrix\n",
    "tfidf, matrix = get_matrix(train_df['text'].to_list()) if cfg['TF_IDF'] else (None, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading projection weights from 6 Gensim Continuous Skipgram/model.bin\n",
      "INFO : KeyedVectors lifecycle event {'msg': 'loaded (302866, 300) matrix of type float32 from 6 Gensim Continuous Skipgram/model.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-02-14T15:38:59.523633', 'gensim': '4.3.0', 'python': '3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]', 'platform': 'Linux-6.1.11-1-MANJARO-x86_64-with-glibc2.37', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE MODEL\n",
    "from nltk.tokenize import word_tokenize\n",
    "w2v_file = f\"{cfg['MODEL']}/model.bin\"\n",
    "model: gensim.models.keyedvectors.KeyedVectors = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "VECTOR_SIZE = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "Calculating mean review vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [02:16<00:00, 182.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.vectorizers import get_tfidf_vector\n",
    "\n",
    "def text_vector(text: str, text_id: int) -> np.ndarray:\n",
    "    \"\"\"Compute the normalized weighted mean w2v vector for a given text\"\"\"\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token in model]\n",
    "    # Compute the word2vec vectors for each word in the text\n",
    "    vectors = [model.get_vector(tkn) for tkn in tokens]\n",
    "    if cfg['TF_IDF']:\n",
    "        # Compute the tf-idf values for each word in the text.\n",
    "        tfidf_vals = get_tfidf_vector(tokens, text_id, matrix, tfidf)\n",
    "        # Compute the weighted vectors by multiplying the word2vec vectors by the tf-idf values\n",
    "        weighted_vecs = np.array([vec * factor for vec, factor in zip(vectors, tfidf_vals)])\n",
    "    else:\n",
    "        weighted_vecs = np.array(vectors)\n",
    "\n",
    "    text_sum_vec = np.sum(weighted_vecs, axis=0)\n",
    "    norm_vec = text_sum_vec / np.linalg.norm(text_sum_vec)\n",
    "    \n",
    "    return norm_vec\n",
    "\n",
    "review_vectors = np.empty((INPUT_DATASET_SIZE, VECTOR_SIZE), dtype=float)\n",
    "print(review_vectors.shape)\n",
    "print(\"Calculating mean review vectors...\")\n",
    "for row_id in tqdm(range(INPUT_DATASET_SIZE)):\n",
    "    review_vectors[row_id] = text_vector(train_df.loc[row_id, \"text\"], row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [03:44<00:00, 111.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# concatenating tfidf and w2v vectors for each text\n",
    "import src.vectorizers as vecs  \n",
    "conc_matrix = vecs.concat_w2v(review_vectors, matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total answers: 25000\n",
      "    Total reviews: 25000\n",
      "    Review vector size: 144680\n",
      "    Answers splitted: 20000 / 5000\n",
      "    Input splitted: 20000 / 5000\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "answers = train_df['answer'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    conc_matrix,\n",
    "    answers,\n",
    "    test_size=cfg['TEST_RATIO']\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "    Total answers: {answers.shape[0]}\n",
    "    Total reviews: {conc_matrix.shape[0]}\n",
    "    Review vector size: {conc_matrix.shape[1]}\n",
    "    Answers splitted: {y_train.shape[0]} / {y_test.shape[0]}\n",
    "    Input splitted: {X_train.shape[0]} / {X_test.shape[0]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Score on\n",
      "      - train  data : 0.92635\n",
      "      - test   data : 0.8714\n",
      "    Test/train size : 0.2\n",
      "    Total time      : 12m 16s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pprint\n",
    "\n",
    "reg : LogisticRegression = LogisticRegression(max_iter=10_000).fit(X_train, y_train)\n",
    "\n",
    "diff = time() - b_time\n",
    "\n",
    "results = {}\n",
    "results.update(cfg)\n",
    "print(f\"\"\"\n",
    "    Score on\n",
    "      - train  data : {reg.score(X_train, y_train)}\n",
    "      - test   data : {reg.score(X_test, y_test)}\n",
    "    Test/train size : {cfg['TEST_RATIO']}\n",
    "    Total time      : {\"{:.0f}m {:.0f}s\".format(*divmod(diff, 60))}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 300 features, but LogisticRegression is expecting 144680 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     text \u001b[39m=\u001b[39m test_df\u001b[39m.\u001b[39mloc[row_id, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m     test_df\u001b[39m.\u001b[39mloc[row_id, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m prep\u001b[39m.\u001b[39mpreprocess(\n\u001b[1;32m     21\u001b[0m         text, cfg, freq_dict, token_amount_counter\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m     result\u001b[39m.\u001b[39mloc[row_id, \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(reg\u001b[39m.\u001b[39;49mpredict([get_vector(text)])[\u001b[39m0\u001b[39m])\n\u001b[1;32m     24\u001b[0m test_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_cleaned.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 300 features, but LogisticRegression is expecting 144680 features as input."
     ]
    }
   ],
   "source": [
    "def get_vector(text: str) -> np.ndarray:\n",
    "    tokens = [token for token in word_tokenize(text) if token in model]\n",
    "    # Compute the word2vec vectors for each word in the text\n",
    "    vectors = np.array([model.get_vector(tkn) for tkn in tokens])\n",
    "    sum_vector = np.sum(vectors, axis=0)\n",
    "    norm_vector = sum_vector / np.linalg.norm(sum_vector)\n",
    "    return norm_vector\n",
    "\n",
    "test_file = \"test.csv\"\n",
    "test_df = pd.read_csv(test_file)\n",
    "TEST_DATASET_SIZE = test_df.shape[0]\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "#             Processing            #\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "print(\"Processing...\")\n",
    "result = test_df[[\"id\"]].copy()\n",
    "for row_id in tqdm(range(TEST_DATASET_SIZE)):\n",
    "    text = test_df.loc[row_id, \"text\"]\n",
    "    test_df.loc[row_id, \"text\"] = prep.preprocess(\n",
    "        text, cfg, freq_dict, token_amount_counter\n",
    "    )\n",
    "    result.loc[row_id, \"answer\"] = int(reg.predict([get_vector(text)])[0])\n",
    "test_df.to_csv(f\"test_cleaned.csv\", index=False)\n",
    "result[\"answer\"] = result[\"answer\"].astype(int)\n",
    "result.to_csv(f\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
